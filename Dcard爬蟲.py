!pip uninstall undetected-chromedriver -y
!pip install selenium webdriver-manager

%pip install undetected-chromedriver

import pandas as pd

%pip install keyboard

import subprocess
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from time import sleep
from datetime import datetime, timedelta
from tqdm import tqdm

# ----------------------------
# é…ç½®åƒæ•¸
# ----------------------------
CONFIG = {
    "wait_timeout": 1,       # é¡¯å¼ç­‰å¾…è¶…æ™‚ (ç§’)
    "short_wait": 1,         # çŸ­æš«ç­‰å¾… (ç§’)
    "medium_wait": 1,        # ä¸­ç­‰ç­‰å¾… (ç§’)
    "max_workers": 5,        # æœ€å¤§ä¸¦ç™¼ç·šç¨‹æ•¸
    "max_scroll": 1000,      # æœ€å¤šæ»¾å‹•æ¬¡æ•¸
}

# ----------------------------
# è‡ªå‹•åµæ¸¬ Chrome ä¸»ç‰ˆæœ¬
# ----------------------------
def get_chrome_version():
    try:
        version = subprocess.check_output(
            r'reg query "HKEY_CURRENT_USER\Software\Google\Chrome\BLBeacon" /v version',
            shell=True
        ).decode("utf-8").strip().split()[-1]
        return int(version.split(".")[0])
    except Exception as e:
        print("âŒ ç„¡æ³•æª¢æŸ¥ Chrome ç‰ˆæœ¬ï¼Œè«‹ç¢ºèªå·²å®‰è£ Chrome")
        raise e

chrome_major_version = get_chrome_version()
print(f"âœ… åµæ¸¬åˆ° Chrome ä¸»ç‰ˆæœ¬è™Ÿ: {chrome_major_version}")

# ----------------------------
# å•Ÿå‹•ç€è¦½å™¨
# ----------------------------
options = uc.ChromeOptions()
options.add_argument("--disable-blink-features=AutomationControlled")
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")
options.add_argument("--window-size=1920,1080")
options.add_argument("--incognito")
options.add_argument("--blink-settings=imagesEnabled=false")  # ç¦ç”¨åœ–ç‰‡
options.add_experimental_option("prefs", {
    "profile.managed_default_content_settings.images": 2,
    "profile.default_content_setting_values.notifications": 2
})
driver = uc.Chrome(version_main=chrome_major_version, options=options)

processed_links = set()

# ----------------------------
# æŠ“å–æ–‡ç« é€£çµ (ç›´åˆ°ä¸‰å¹´å‰)
# ----------------------------
def get_article_links(driver, board_name, max_scroll=CONFIG["max_scroll"]):
    article_links, seen_links = [], set()
    driver.get(f"https://www.dcard.tw/f/{board_name}")
    try:
        WebDriverWait(driver, CONFIG["wait_timeout"]).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        sleep(CONFIG["medium_wait"])
    except TimeoutException:
        print(f"âš ï¸ {board_name} é é¢åŠ è¼‰è¶…æ™‚")

    cutoff_date = datetime.now() - timedelta(days=365*10)  # âœ… åå¹´å‰
    for i in range(max_scroll):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        sleep(CONFIG["short_wait"])

        posts = driver.find_elements(By.CSS_SELECTOR, "a[href*='/p/']")
        for post in posts:
            link = post.get_attribute("href")
            if not link or link in seen_links:
                continue
            seen_links.add(link)

            # å˜—è©¦æŠ“æ–‡ç« æ—¥æœŸ
            try:
                time_el = post.find_element(By.XPATH, ".//time")
                date_text = time_el.get_attribute("datetime") or time_el.text
                post_date = datetime.fromisoformat(date_text.replace("Z", "+00:00")).replace(tzinfo=None)
            except:
                post_date = datetime.now()

            if post_date >= cutoff_date:
                article_links.append(link)
            else:
                print("âš ï¸ å·²ç¶“é‡åˆ°è¶…éŽåå¹´çš„æ–‡ç«  â†’ åœæ­¢æ»¾å‹•")
                return article_links
    return article_links

# ----------------------------
# æŠ“å–æ–‡ç« å…§å®¹ (å«è©•è«–)
# ----------------------------
def get_article_content(url):
    if url in processed_links:
        return None
    try:
        driver.get(url)
        wait = WebDriverWait(driver, CONFIG["wait_timeout"])

        # æ¨™é¡Œ
        title = ""
        for selector in ["h1", "[data-testid='article-title']", "title"]:
            try:
                if selector == "title":
                    raw = driver.title
                    if raw and "Dcard" in raw:
                        title = raw.replace(" | Dcard", "").replace("- Dcard", "").strip()
                else:
                    title_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                    t = title_element.text.strip()
                    if t and t != "è«‹ç¨å€™...":
                        title = t
                if title:
                    break
            except Exception:
                continue

        # å…§å®¹
        content = ""
        for selector in ["div[data-testid='post-content']", "article", ".content"]:
            try:
                content_element = wait.until(
                    lambda d: d.find_element(By.CSS_SELECTOR, selector)
                    if d.find_element(By.CSS_SELECTOR, selector).text.strip() != "è«‹ç¨å€™..." else False
                )
                text = content_element.text.strip()
                if text and text != "è«‹ç¨å€™..." and len(text) > 10:
                    content = text
                    break
            except Exception:
                continue

        # æ—¥æœŸ
        try:
            date_element = driver.find_element(By.CSS_SELECTOR, 'time, [data-testid="post-date"]')
            date_text = date_element.get_attribute("datetime") or date_element.text
            post_date = datetime.fromisoformat(date_text.replace("Z", "+00:00")).replace(tzinfo=None)
        except Exception:
            post_date = datetime.now()

        # ä¸‰å¹´å…§æ–‡ç« 
        if post_date < datetime.now() - timedelta(days=365*3):
            return None

        # æŠ“è©•è«–
        comments = []
        try:
            comment_elements = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid="comment"]')
            for c in comment_elements:
                txt = c.text.strip()
                if txt:
                    comments.append(txt)
        except Exception:
            pass

        processed_links.add(url)
        return {
            "æ¨™é¡Œ": title,
            "å…§å®¹": content,
            "é€£çµ": url,
            "æ—¥æœŸ": post_date.strftime("%Y-%m-%d"),
            "è©•è«–": " || ".join(comments) if comments else ""
        }
    except Exception:
        return None

# ----------------------------
# çˆ¬å–å¤šå€‹çœ‹æ¿
# ----------------------------
def crawl_all_boards(boards):
    all_links = {}
    for board_name in boards.keys():
        links = get_article_links(driver, board_name, CONFIG["max_scroll"])
        print(f"âœ… {board_name} å…±ç²å– {len(links)} ç¯‡æ–‡ç« é€£çµ")
        all_links[board_name] = links

    data_by_board = {b: [] for b in boards.keys()}
    total_links = sum(len(v) for v in all_links.values())

    with ThreadPoolExecutor(max_workers=CONFIG["max_workers"]) as executor:
        futures = []
        for board_name, links in all_links.items():
            for link in links:
                futures.append((board_name, executor.submit(get_article_content, link)))

        for board_name, future in tqdm(futures, total=total_links, desc="æŠ“å–æ–‡ç« é€²åº¦"):
            result = future.result()
            if result:
                data_by_board[board_name].append(result)

    for board_name, filename in boards.items():
        if data_by_board[board_name]:
            pd.DataFrame(data_by_board[board_name]).to_csv(filename, index=False, encoding="utf-8-sig")
            print(f"ðŸ“‚ {filename} å·²ä¿å­˜ {len(data_by_board[board_name])} ç¯‡æ–‡ç« ")

# ----------------------------
# ä¸»ç¨‹å¼
# ----------------------------
boards = {
    "travel": "æ—…éŠ.csv",
     "food": "ç¾Žé£Ÿ.csv",
     "job": "å·¥ä½œ.csv",
     "graduate_school": "ç ”ç©¶æ‰€.csv",
     "exam": "è€ƒè©¦.csv"
}

try:
    crawl_all_boards(boards)
except Exception as e:
    print(f"âŒ ä¸»ç¨‹å¼éŒ¯èª¤: {e}")
finally:
    driver.quit()
    print("ðŸ›‘ ç€è¦½å™¨å·²é—œé–‰")


import pandas as pd
import os

# ----------------------------
# çœ‹æ¿å°æ‡‰æª”å
# ----------------------------
boards = {
    "travel": "æ—…éŠ.csv",
     "food": "ç¾Žé£Ÿ.csv",
     "job": "å·¥ä½œ.csv",
    "graduate_school": "ç ”ç©¶æ‰€.csv",
     "exam": "è€ƒè©¦.csv"
}

# ----------------------------
# è¦æ¸…é™¤çš„ Dcard ç³»çµ±å­—æ¨£
# ----------------------------
remove_phrases = [
    "Dcard éœ€è¦ç¢ºèªæ‚¨çš„é€£ç·šæ˜¯å®‰å…¨çš„"
]

# ----------------------------
# CSV æ¸…ç†å‡½æ•¸
# ----------------------------
def clean_csv(file_path, output_path):
    # å˜—è©¦è®€å– CSVï¼Œå³ä½¿æ¬„ä½æ•¸ä¸é½Šä¹Ÿè®€é€²ä¾†
    df = pd.read_csv(file_path, on_bad_lines="skip")

    # ç§»é™¤ã€Œé€£çµã€æ¬„ä½
    df_cleaned = df.drop(columns=["é€£çµ"], errors="ignore").copy()

    # ç§»é™¤èˆŠçš„ã€Œç·¨è™Ÿã€æ¬„ä½ï¼ˆè‹¥å­˜åœ¨ï¼‰
    if "ç·¨è™Ÿ" in df_cleaned.columns:
        df_cleaned = df_cleaned.drop(columns=["ç·¨è™Ÿ"])

    # åŽ»æŽ‰ NaN åˆ—ï¼ˆç¼ºæ¬„ä½ã€ç©ºå€¼ï¼‰
    df_cleaned = df_cleaned.dropna(how="any")

    # æ¸…ç†ã€Œæ¨™é¡Œã€ã€Œå…§å®¹ã€
    # ç§»é™¤ã€Œè«‹ç¨å€™...ã€çš„åˆ—
    for col in ["æ¨™é¡Œ", "å…§å®¹"]:
        if col in df_cleaned.columns:
            df_cleaned = df_cleaned[df_cleaned[col].str.strip() != "è«‹ç¨å€™..."]

            df_cleaned[col] = (
                df_cleaned[col]
                .astype(str)
                .str.replace(r"\s+", " ", regex=True)  # ç§»é™¤å¤šé¤˜ç©ºç™½
                .apply(lambda x: "".join(x.replace(p, "") for p in remove_phrases))
                .str.strip()
            )

    # ç§»é™¤æ¸…ç†å¾Œè®Šæˆç©ºå­—ä¸²çš„åˆ—
    if "æ¨™é¡Œ" in df_cleaned.columns and "å…§å®¹" in df_cleaned.columns:
        df_cleaned = df_cleaned[
            (df_cleaned["æ¨™é¡Œ"].str.strip() != "") &
            (df_cleaned["å…§å®¹"].str.strip() != "")
        ]

    # é‡æ–°ç”Ÿæˆæµæ°´ç·¨è™Ÿ
    df_cleaned.insert(0, "ç·¨è™Ÿ", range(1, len(df_cleaned) + 1))

    # è¼¸å‡ºæ¸…ç†å¾Œçš„æª”æ¡ˆ
    df_cleaned.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²è™•ç†å®Œæˆ: {output_path}")

# ----------------------------
# æ‰¹æ¬¡è™•ç†
# ----------------------------
for board, filename in boards.items():
    if os.path.exists(filename):
        output_file = f"{os.path.splitext(filename)[0]}.csv"
        clean_csv(filename, output_file)
    else:
        print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {filename}")



if __name__ == "__main__":
    # é€™è£¡å¯ä»¥æ”¾ä¸»è¦åŸ·è¡Œçš„å‡½å¼æˆ–æµç¨‹
    # ä¾‹å¦‚ main() æˆ–å…¶ä»–å•Ÿå‹•ç¨‹å¼ç¢¼
    print("Dcardçˆ¬èŸ²é–‹å§‹åŸ·è¡Œ...")
